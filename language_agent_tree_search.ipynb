{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "WNSy5ZpSx63L",
      "metadata": {
        "id": "WNSy5ZpSx63L"
      },
      "source": [
        "# **LATS Implementation without External Web Search Tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3tB8RvsvxVgi",
      "metadata": {
        "id": "3tB8RvsvxVgi"
      },
      "source": [
        "# 1. Setting Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ree_9koxSHc",
      "metadata": {
        "id": "_ree_9koxSHc"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from collections import deque\n",
        "from typing import Optional, Literal\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers.openai_tools import (\n",
        "    JsonOutputToolsParser,\n",
        "    PydanticToolsParser,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.runnables import chain as as_runnable\n",
        "from langchain_core.prompt_values import ChatPromptValue\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from IPython.display import Image, display, Markdown\n",
        "from collections import defaultdict\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"\") #enter open-Ai key here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efayTQQDyzwE",
      "metadata": {
        "id": "efayTQQDyzwE"
      },
      "source": [
        "# 2. Class Declarations of Node, Tree State and Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "CuDsAhB1y3uR",
      "metadata": {
        "id": "CuDsAhB1y3uR"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(\n",
        "        self,\n",
        "        messages: list[BaseMessage],\n",
        "        reflection: Reflection,\n",
        "        parent: Optional[Node] = None,\n",
        "    ):\n",
        "        self.messages = messages\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.value = 0\n",
        "        self.visits = 0\n",
        "        self.reflection = reflection\n",
        "        self.depth = parent.depth + 1 if parent is not None else 1\n",
        "        self._is_solved = reflection.found_solution if reflection else False\n",
        "        if self._is_solved:\n",
        "            self._mark_tree_as_solved()\n",
        "        self.backpropagate(reflection.normalized_score)\n",
        "        print(f\"Created node : {self}\")\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"<Node value={self.value:.2f}, visits={self.visits},\"\n",
        "            f\" Response={self.messages[-1].content[:50] if self.messages else 'No messages'}...,\"\n",
        "            f\" Reflection={self.reflection.reflections[:50] if self.reflection else 'No reflection'}...,\"\n",
        "            f\" is_solved={self._is_solved}, depth={self.depth}>\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_solved(self):\n",
        "        return self._is_solved\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        return not self.children\n",
        "\n",
        "    @property\n",
        "    def best_child(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        all_nodes = self._get_all_children()\n",
        "        return max(all_nodes, key=lambda child: child.upper_confidence_bound())\n",
        "\n",
        "    @property\n",
        "    def best_child_score(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
        "\n",
        "    @property\n",
        "    def height(self) -> int:\n",
        "        if self.children:\n",
        "            return 1 + max([child.height for child in self.children])\n",
        "        return 1\n",
        "\n",
        "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
        "        if self.parent is None:\n",
        "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
        "        if self.visits == 0:\n",
        "            return float('inf')\n",
        "        average_reward = self.value / self.visits\n",
        "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
        "        return average_reward + exploration_weight * exploration_term\n",
        "\n",
        "    def backpropagate(self, reward: float):\n",
        "        node = self\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
        "            node = node.parent\n",
        "\n",
        "    def get_messages(self, include_reflections: bool = True):\n",
        "        if include_reflections:\n",
        "            return self.messages + [self.reflection.as_message()]\n",
        "        return self.messages\n",
        "\n",
        "    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:\n",
        "        messages = []\n",
        "        node = self\n",
        "        while node:\n",
        "            messages.extend(\n",
        "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
        "            )\n",
        "            node = node.parent\n",
        "        return messages[::-1]\n",
        "\n",
        "    def _get_all_children(self):\n",
        "        all_nodes = []\n",
        "        nodes = deque([self])\n",
        "        while nodes:\n",
        "            node = nodes.popleft()\n",
        "            all_nodes.extend(node.children)\n",
        "            nodes.extend(node.children)\n",
        "        return all_nodes\n",
        "\n",
        "    def get_best_solution(self):\n",
        "        all_nodes = [self] + self._get_all_children()\n",
        "        best_node = max(\n",
        "            all_nodes,\n",
        "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
        "        )\n",
        "        return best_node\n",
        "\n",
        "    def _mark_tree_as_solved(self):\n",
        "        parent = self.parent\n",
        "        while parent:\n",
        "            parent._is_solved = True\n",
        "            parent = parent.parent\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    reflections: str = Field(\n",
        "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
        "        \" and general quality of the response.\"\n",
        "    )\n",
        "    score: int = Field(\n",
        "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
        "        ge=0,\n",
        "        le=10,\n",
        "    )\n",
        "\n",
        "    found_solution: bool = Field(\n",
        "        description=\"Whether the response has fully and perfectly solved the question or task.\\\n",
        "         This should never be true unless an except exceptional answer is generated\")\n",
        "\n",
        "    def as_message(self):\n",
        "        return HumanMessage(\n",
        "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def normalized_score(self) -> float:\n",
        "        return self.score / 10.0\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class TreeState(TypedDict):\n",
        "    root: Node\n",
        "    input: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W5uJOV962jje",
      "metadata": {
        "id": "W5uJOV962jje"
      },
      "source": [
        "# 3. Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "hE-JCSBkzPee",
      "metadata": {
        "id": "hE-JCSBkzPee"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Reflect and grade the assistant response to the user question below. \\\n",
        "             Be highly critical in response and dont be satisfied easily\\\n",
        "             Check for following critera 1. Relevance to the question 2. Factual Correctness 3. Quality of text\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"candidate\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "reflection_llm_chain = (\n",
        "    prompt\n",
        "    | llm.bind_tools(tools=[Reflection], tool_choice=\"Reflection\").with_config(\n",
        "        run_name=\"Reflection\"\n",
        "    )\n",
        "    | PydanticToolsParser(tools=[Reflection])\n",
        ")\n",
        "@as_runnable\n",
        "def reflection_chain(inputs) -> Reflection:\n",
        "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
        "    reflection = tool_choices[0]\n",
        "    if not isinstance(inputs[\"candidate\"][-1], AIMessage):\n",
        "        reflection.found_solution = False\n",
        "    print(f\"Generated reflection: {reflection} \\n\")\n",
        "    return reflection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4xPlGuiL4tJM",
      "metadata": {
        "id": "4xPlGuiL4tJM"
      },
      "source": [
        "# 4. Initial Response with Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "-e8FY8-330c_",
      "metadata": {
        "id": "-e8FY8-330c_"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an AI assistant. Your job is to answer user question in an accurate and concise manner \",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "initial_answer_chain = prompt_template | llm.with_config(run_name=\"GenerateInitialCandidate\")\n",
        "\n",
        "parser = JsonOutputToolsParser(return_id=True)\n",
        "\n",
        "def generate_initial_response(state: TreeState) -> dict:\n",
        "    print(\"Generating initial response\")\n",
        "    res = initial_answer_chain.invoke({\"input\": state[\"input\"]})\n",
        "    output_messages = [res]\n",
        "    content = res.content\n",
        "    display(Markdown(content))\n",
        "    # print(f\"Initial response: {res.content[:100]}...\")\n",
        "    reflection = reflection_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
        "    )\n",
        "    # print(f\"\\nInitial reflection: {reflection} \\n \")\n",
        "    root = Node(output_messages, reflection=reflection)\n",
        "    print(f\"Initial root node created: {root}\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"root\": root,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0E396u6r6Feg",
      "metadata": {
        "id": "0E396u6r6Feg"
      },
      "source": [
        "# 5. Tree Expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4ixbedUf6KI2",
      "metadata": {
        "id": "4ixbedUf6KI2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
        "    n = config[\"configurable\"].get(\"N\", 5)\n",
        "    print(f\"Generating {n} candidates\")\n",
        "    chat_result = llm.generate(\n",
        "        [messages.to_messages()],\n",
        "        n=n,\n",
        "        callbacks=config[\"callbacks\"],\n",
        "        run_name=\"GenerateCandidates\"\n",
        "    )\n",
        "    return [gen.message for gen in chat_result.generations[0]]\n",
        "\n",
        "expansion_chain = prompt_template | generate_candidates\n",
        "\n",
        "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
        "    print(\"Expanding tree \\n\")\n",
        "    root = state[\"root\"]\n",
        "    best_candidate: Node = root.best_child if root.children else root\n",
        "    print(f\"Best candidate for expansion : {best_candidate} \\n\")\n",
        "    messages = best_candidate.get_trajectory()\n",
        "\n",
        "    new_candidates = expansion_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
        "    )\n",
        "    print(f\"Generated {len(new_candidates)} new candidates \\n\")\n",
        "\n",
        "    output_messages = [[candidate] for candidate in new_candidates]\n",
        "\n",
        "    reflections = reflection_chain.batch(\n",
        "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    child_nodes = [\n",
        "        Node(cand, parent=best_candidate, reflection=reflection)\n",
        "        for cand, reflection in zip(output_messages, reflections)\n",
        "    ]\n",
        "    best_candidate.children.extend(child_nodes)\n",
        "    print(f\"\\n Added {len(child_nodes)} child nodes to the tree \\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def should_loop(state: TreeState) -> Literal[\"expand\", \"__end__\"]:\n",
        "    root = state[\"root\"]\n",
        "    print(f\"Checking if should loop again. Root height: {root.height}, Solution Found: {root.is_solved} \\n\")\n",
        "    if root.is_solved:\n",
        "        print(\"Root is solved. Ending search. \\n\")\n",
        "        return END\n",
        "    if root.height > 5:\n",
        "        print(\"Max height reached. Ending search. \\n \")\n",
        "        return END\n",
        "    print(\"Continuing to expand. \\n\")\n",
        "    return \"expand\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RoUitOtU63UP",
      "metadata": {
        "id": "RoUitOtU63UP"
      },
      "source": [
        "# 6. Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "XwJALZMv664F",
      "metadata": {
        "id": "XwJALZMv664F"
      },
      "outputs": [],
      "source": [
        "\n",
        "builder = StateGraph(TreeState)\n",
        "builder.add_node(\"start\", generate_initial_response)\n",
        "builder.add_node(\"expand\", expand)\n",
        "builder.add_edge(START, \"start\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"start\",\n",
        "    should_loop,\n",
        ")\n",
        "builder.add_conditional_edges(\n",
        "    \"expand\",\n",
        "    should_loop,\n",
        ")\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xl2F_oB-7GG6",
      "metadata": {
        "id": "xl2F_oB-7GG6"
      },
      "source": [
        "# 7. Tree Search for best answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Iwtu9RrT7JTo",
      "metadata": {
        "id": "Iwtu9RrT7JTo"
      },
      "outputs": [],
      "source": [
        "def print_tree(node, level=0):\n",
        "    print(\"  \" * level + str(node))\n",
        "    for child in node.children:\n",
        "        print_tree(child, level + 1)\n",
        "\n",
        "def run_tree_search(question):\n",
        "    print(f\"Starting tree search for question\")\n",
        "    last_step = None\n",
        "    for step in graph.stream({\"input\": question}):\n",
        "        last_step = step\n",
        "        step_name, step_state = next(iter(step.items()))\n",
        "        print(f\"Step: {step_name}\")\n",
        "        print(f\"Tree height: {step_state['root'].height}\")\n",
        "        print(\"--------------------------------------------------------\")\n",
        "\n",
        "    if \"expand\" in last_step:\n",
        "        solution_node = last_step[\"expand\"][\"root\"].get_best_solution()\n",
        "        best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
        "        print(\"Best solution found:\")\n",
        "        # print(best_trajectory[-1].content)\n",
        "        content = best_trajectory[-1].content\n",
        "        display(Markdown(content))\n",
        "    else:\n",
        "        print(\"Tree expansion ended \\n \")\n",
        "\n",
        "    print(\"Final tree structure:\")\n",
        "    print_tree(last_step[\"start\"][\"root\"] if \"start\" in last_step else last_step[\"expand\"][\"root\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0MH8hv6y7VQ8",
      "metadata": {
        "id": "0MH8hv6y7VQ8"
      },
      "source": [
        "# 8. Test query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "iJ4RyHjx7P2J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJ4RyHjx7P2J",
        "outputId": "6285219e-d3a3-4017-92dc-8b45bc14dd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting tree search for question\n",
            "Generating initial response\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "To swap ORAI for USDT using the Keplr Wallet, follow the steps below. Ensure you have enough funds and pay attention to all transaction fees.\n",
              "\n",
              "1. Install Keplr Wallet:\n",
              "   - If you don't already have it, install the Keplr Wallet extension on your browser. It's available on Chrome, Firefox, and Brave.\n",
              "\n",
              "2. Create/Access your Account:\n",
              "   - If you're a new user, set up your Keplr Wallet by creating a new account and safely storing your mnemonic phrase. If you're an existing user, log in to your Keplr wallet.\n",
              "\n",
              "3. Add ORAI Token:\n",
              "   - If you haven't done so, you'll need to add the ORAI token to your Keplr wallet. On the dashboard, there should be an “Add Token” option. Assuming you've already got ORAI tokens in your wallet, skip to step 4.\n",
              "\n",
              "4. Navigate to Swap Interface:\n",
              "   - Within the Keplr wallet extension, there should be an interface for swapping.\n",
              "\n",
              "5. Select Tokens:\n",
              "   - In the swap section, select ORAI to sell and USDT to buy.\n",
              "\n",
              "6. Specify Amount:\n",
              "   - Enter the amount of ORAI you wish to swap (0.1 ORAI as per your question).\n",
              "\n",
              "7. Confirm Swap Details:\n",
              "   - Review all details of the swap, such as amount, rate, and any applicable fees, and then confirm the swap.\n",
              "\n",
              "8. Transaction Confirmation:\n",
              "   - Once you've confirmed your swap, your Keplr Wallet will carry out the transaction. The USDT will be added to your wallet once the transaction has been processed on the blockchain.\n",
              "\n",
              "9. Check Balance:\n",
              "   - Finally, after the transaction is complete, you can check your new USDT balance on the dashboard of your Keplr Wallet.\n",
              "\n",
              "Remember that transaction times depend on the current state of the network and the gas price you set. Always make sure to double-check all transaction details before confirming."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated reflection: reflections=\"The assistant's response was clear, concise, and useful. It walked the user step-by-step through the process, making sure to explain things in simple terms and ensuring to cover all crucial details. Moreover, it also provided valuable additional information about being cautious about potential transaction fees, storing the mnemonic phrase safely, and double-checking transaction details before confirming. This could help the user avoid potential mistakes. The correctness of the instructions depends entirely on the interface and abilities of the Keplr Wallet, as it's a third-party application and its features may vary, but based on common knowledge of similar applications, the instructions provided seem factual and relevant.\" score=9 found_solution=True \n",
            "\n",
            "Created node : <Node value=0.90, visits=1, Response=To swap ORAI for USDT using the Keplr Wallet, foll..., Reflection=The assistant's response was clear, concise, and u..., is_solved=True, depth=1>\n",
            "Initial root node created: <Node value=0.90, visits=1, Response=To swap ORAI for USDT using the Keplr Wallet, foll..., Reflection=The assistant's response was clear, concise, and u..., is_solved=True, depth=1>\n",
            "Checking if should loop again. Root height: 1, Solution Found: True \n",
            "\n",
            "Root is solved. Ending search. \n",
            "\n",
            "Step: start\n",
            "Tree height: 1\n",
            "--------------------------------------------------------\n",
            "Tree expansion ended \n",
            " \n",
            "Final tree structure:\n",
            "<Node value=0.90, visits=1, Response=To swap ORAI for USDT using the Keplr Wallet, foll..., Reflection=The assistant's response was clear, concise, and u..., is_solved=True, depth=1>\n"
          ]
        }
      ],
      "source": [
        "question = \"swap 0.1 Orai for usdt using keplr wallet\"\n",
        "run_tree_search(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJbAf2WdAc8E",
      "metadata": {
        "id": "pJbAf2WdAc8E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
